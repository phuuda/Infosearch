{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Sofia/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#    ulimit -n 2048\n",
    "from collections import Counter, defaultdict\n",
    "import json, os, re, string, operator\n",
    "from pymystem3 import Mystem\n",
    "from math import log\n",
    "\n",
    "def reverse_id(collection):\n",
    "    dic = defaultdict(set) # wont create key duplicates\n",
    "    for i, doc in enumerate(collection):\n",
    "        for item in doc:\n",
    "            dic[item].add(i)\n",
    "    return dic\n",
    "\n",
    "def lemmatize_collection(path):\n",
    "\n",
    "    all_files = os.listdir(path)\n",
    "    text_id_list = {}\n",
    "    all_texts = []\n",
    "    m = Mystem()\n",
    "    j = 0   \n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                \n",
    "                full_file_path = os.path.join(root, file_name)\n",
    "\n",
    "                fh = open(full_file_path, 'r')              \n",
    "                content = fh.readlines()\n",
    "                text = content[5]\n",
    "                text = re.sub('\\n', ' ', text)\n",
    "                text = re.sub(r'[\\.\\?\\!…]([А-Я])', r'. \\1', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub('  ', ' ', text)\n",
    "                text = text.lower()\n",
    "\n",
    "                lemmas = m.lemmatize(text)\n",
    "                lemma_text = ''.join(lemmas)\n",
    "                lemma_text = re.sub('  ', ' ', lemma_text)\n",
    "\n",
    "                words = lemma_text.split()\n",
    "                all_texts.append(words)\n",
    "                text_id_list[j] = file_name\n",
    "                j += 1 \n",
    "                \n",
    "                fh.close()\n",
    "            \n",
    "    return [all_texts, text_id_list]\n",
    "\n",
    "# ----------------------------- query preparation -----------------------------\n",
    "def prepare_query(query): # lowercase, strip punct, lemmatize, remove stop-words\n",
    "    \n",
    "    stop_words = []\n",
    "    \n",
    "    f3 = open('/Users/Sofia/Desktop/inza-vpered/stop_words.txt') # machine-specific\n",
    "    new_ones = [line.rstrip('\\n') for line in f3]\n",
    "    for new in new_ones:\n",
    "        if new not in stop_words:\n",
    "            stop_words.append(new)\n",
    "            \n",
    "    m = Mystem()\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "\n",
    "    q_lemmas = m.lemmatize(query)\n",
    "    q_lemma_text = ''.join(q_lemmas)\n",
    "    query_words = q_lemma_text.split()\n",
    "    final_query = []\n",
    "\n",
    "    for word in query_words:\n",
    "        z = 0\n",
    "        for stop in stop_words:\n",
    "            if word == stop:\n",
    "                z += 1\n",
    "\n",
    "        if z == 0:\n",
    "            final_query.append(word)\n",
    "            \n",
    "    return final_query\n",
    "\n",
    "\n",
    "# ----------------------------- BM 25 -----------------------------\n",
    "def compute_avgdl(all_texts): # avgdl\n",
    "    N = len(all_texts)\n",
    "    sum_text_len = 0\n",
    "    for text in all_texts:\n",
    "        sum_text_len += len(text)\n",
    "        \n",
    "    avgdl = sum_text_len / N\n",
    "    avgdl = round(avgdl, 0)    \n",
    "    return avgdl\n",
    "\n",
    "def compute_n(q, reverse_ids):    # n = n(qi) - num of texts that have qi (qi - word in query)\n",
    "    if q in reverse_ids:\n",
    "        return len(reverse_ids[q])  \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def compute_fq(q, text): # fq - frequency of word qi in document\n",
    "    fq = 0\n",
    "    for word in text:\n",
    "        if q == word:\n",
    "            fq += 1      \n",
    "    return fq\n",
    "\n",
    "def compute_K(dl, avdl):\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)))            \n",
    "\n",
    "def score_BM25(n, fq, N, dl, avgdl): # computes BM25 for 1 query word and 1 document\n",
    "    \n",
    "    k1 = 2.0\n",
    "    K = compute_K(dl, avgdl)\n",
    "    IDF = log((N - n + 0.5) / (n + 0.5))\n",
    "    frac = ((k1 + 1) * fq) / (K + fq)\n",
    "    return IDF * frac\n",
    "\n",
    "\n",
    "# ------------- find document matches (has at least 1 word from query) ------------- \n",
    "def match_docs(final_query, reverse_ids): # find document matches (has at least 1 word from query)\n",
    "    matching_docs = []\n",
    "    for word in final_query:\n",
    "        if word in reverse_ids:\n",
    "            matches = reverse_ids[word]\n",
    "            for i in matches:\n",
    "                if i not in matching_docs:\n",
    "                    matching_docs.append(i)\n",
    "    \n",
    "    return matching_docs\n",
    "\n",
    "\n",
    "# ------------- lemmatized text -> actual text + link ------------- \n",
    "def get_actual_text(reverse_text_id, text_ids):\n",
    "    get_text = text_ids[reverse_text_id]\n",
    "    f4 = open('/Users/Sofia/Desktop/inza-vpered/texts/' + get_text, 'r') # machine-specific\n",
    "    content = f4.readlines()\n",
    "    actual_text = content[5]\n",
    "    \n",
    "    link_part = re.sub('.txt', '', get_text)\n",
    "    link = 'http://inza-vpered.ru/article/' + link_part\n",
    "    f4.close()\n",
    "    \n",
    "    return (actual_text, link)\n",
    "\n",
    "# ------------- compute Okapi BM25 -------------\n",
    "def get_BM25(text, final_query, all_texts): # for 1 text\n",
    "                                      \n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    \n",
    "    N = len(all_texts)\n",
    "    avgdl = compute_avgdl(all_texts)\n",
    "    dl = len(text)\n",
    "\n",
    "    sum_score = 0\n",
    "    \n",
    "    for q in final_query: # for each word in query\n",
    "        n = compute_n(q, reverse_ids) # num of texts that have qi (qi - word in query)\n",
    "        fq = compute_fq(q, text) # frequency of word q in document\n",
    "        score = score_BM25(n, fq, N, dl, avgdl) # computes BM25 for 1 query word and 1 document        \n",
    "        sum_score += score\n",
    "\n",
    "    return sum_score\n",
    "\n",
    "# ------------- final sorting function ------------- \n",
    "def BM25_sort(query, path_to_collection, texts_and_ids, reverse_ids):\n",
    "    \n",
    "    #texts_and_ids = lemmatize_collection(path)\n",
    "    all_texts = texts_and_ids[0]\n",
    "    text_ids = texts_and_ids[1]\n",
    "\n",
    "    #reverse_ids = reverse_id(all_texts)    \n",
    "    final_query = prepare_query(query)\n",
    "    \n",
    "    matches = match_docs(final_query, reverse_ids)\n",
    "    docs_and_scores = {}\n",
    "    \n",
    "    for match in matches: # match -- id\n",
    "        text = all_texts[match]\n",
    "        score = get_BM25(text, final_query, all_texts)\n",
    "        \n",
    "        actual_text = get_actual_text(match, text_ids)\n",
    "        docs_and_scores[actual_text] = score\n",
    "        \n",
    "    sorted_matches = sorted(docs_and_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # return only texts\n",
    "    \n",
    "    return sorted_matches\n",
    "    \n",
    "path = '/Users/Sofia/Desktop/inza-vpered/texts' # machine-specific\n",
    "texts_and_ids = lemmatize_collection(path)\n",
    "all_texts = texts_and_ids[0]\n",
    "reverse_ids = reverse_id(all_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Oct/2017 22:20:13] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "# ii - shut down server\n",
    "# рождество и каникулы\n",
    "\n",
    "from flask import Flask\n",
    "from flask import url_for, render_template, request, redirect\n",
    "from pymystem3 import Mystem\n",
    "import operator, json\n",
    "from collections import Counter\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/', methods=['get', 'post'])\n",
    "def main_one():\n",
    "    \n",
    "    if request.form:\n",
    "        query = str(request.form)\n",
    "    \n",
    "        results = BM25_sort(query, path, texts_and_ids, reverse_ids)\n",
    "    \n",
    "        return render_template('index_page.html', input=query,\n",
    "                               query=query, results=results)\n",
    "    \n",
    "    else:\n",
    "        return render_template('index_page.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
