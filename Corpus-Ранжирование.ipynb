{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infosearch plan:\n",
    "    + get corpus from web\n",
    "    + inverted index\n",
    "    + ранжирование\n",
    "    easy flask app for search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#    ulimit -n 2048\n",
    "from collections import Counter, defaultdict\n",
    "import json, os, re, string, operator\n",
    "from pymystem3 import Mystem\n",
    "from math import log\n",
    "\n",
    "def reverse_id(collection):\n",
    "    dic = defaultdict(set) # wont create key duplicates\n",
    "    for i, doc in enumerate(collection):\n",
    "        for item in doc:\n",
    "            dic[item].add(i)\n",
    "    return dic\n",
    "\n",
    "def lemmatize_collection(path):\n",
    "\n",
    "    all_files = os.listdir(path)\n",
    "    text_id_list = {}\n",
    "    all_texts = []\n",
    "    m = Mystem()\n",
    "    j = 0   \n",
    "\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file_name in files:\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                \n",
    "                full_file_path = os.path.join(root, file_name)\n",
    "\n",
    "                fh = open(full_file_path, 'r')              \n",
    "                content = fh.readlines()\n",
    "                text = content[5]\n",
    "                text = re.sub('\\n', ' ', text)\n",
    "                text = re.sub(r'[\\.\\?\\!…]([А-Я])', r'. \\1', text)\n",
    "                text = re.sub(r'[^\\w\\s]', '', text)\n",
    "                text = re.sub('  ', ' ', text)\n",
    "                text = text.lower()\n",
    "\n",
    "                lemmas = m.lemmatize(text)\n",
    "                lemma_text = ''.join(lemmas)\n",
    "                lemma_text = re.sub('  ', ' ', lemma_text)\n",
    "\n",
    "                words = lemma_text.split()\n",
    "                all_texts.append(words)\n",
    "                text_id_list[j] = file_name\n",
    "                j += 1 \n",
    "\n",
    "                fh.close()\n",
    "            \n",
    "    return [all_texts, text_id_list]\n",
    "\n",
    "#f1 = open('/Users/Sofia/Desktop/inza-vpered/text_ids.txt', 'w')\n",
    "#f2 = open('/Users/Sofia/Desktop/inza-vpered/inverse_ids.txt', 'w')\n",
    "\n",
    "#for k, v in text_ids.items():\n",
    "#    f1.write(\"%s - %s\" % (str(k), str(v)))\n",
    "#    f1.write('\\n')\n",
    "    \n",
    "#for k, v in reverse_ids.items():\n",
    "#    f2.write(\"%s - %s\" % (str(k), str(v)))\n",
    "#    f2.write('\\n')\n",
    "\n",
    "#f1.close()\n",
    "#f2.close()\n",
    "\n",
    "# ----------------------------- query preparation -----------------------------\n",
    "def prepare_query(query): # lowercase, strip punct, lemmatize, remove stop-words\n",
    "    \n",
    "    stop_words = []\n",
    "    \n",
    "    f3 = open('/Users/Sofia/Desktop/inza-vpered/stop_words.txt') # machine-specific\n",
    "    new_ones = [line.rstrip('\\n') for line in f3]\n",
    "    for new in new_ones:\n",
    "        if new not in stop_words:\n",
    "            stop_words.append(new)\n",
    "            \n",
    "    m = Mystem()\n",
    "    query = query.lower()\n",
    "    query = re.sub(r'[^\\w\\s]', '', query)\n",
    "\n",
    "    q_lemmas = m.lemmatize(query)\n",
    "    q_lemma_text = ''.join(q_lemmas)\n",
    "    query_words = q_lemma_text.split()\n",
    "    final_query = []\n",
    "\n",
    "    for word in query_words:\n",
    "        z = 0\n",
    "        for stop in stop_words:\n",
    "            if word == stop:\n",
    "                z += 1\n",
    "\n",
    "        if z == 0:\n",
    "            final_query.append(word)\n",
    "            \n",
    "    return final_query\n",
    "\n",
    "\n",
    "# ----------------------------- BM 25 -----------------------------\n",
    "def compute_avgdl(all_texts): # avgdl\n",
    "    N = len(all_texts)\n",
    "    sum_text_len = 0\n",
    "    for text in all_texts:\n",
    "        sum_text_len += len(text)\n",
    "        \n",
    "    avgdl = sum_text_len / N\n",
    "    avgdl = round(avgdl, 0)    \n",
    "    return avgdl\n",
    "\n",
    "def compute_n(q, reverse_ids):    # n = n(qi) - num of texts that have qi (qi - word in query)\n",
    "    if q in reverse_ids:\n",
    "        return len(reverse_ids[q])  \n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def compute_fq(q, text): # fq - frequency of word qi in document\n",
    "    fq = 0\n",
    "    for word in text:\n",
    "        if q == word:\n",
    "            fq += 1      \n",
    "    return fq\n",
    "\n",
    "def compute_K(dl, avdl):\n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    return k1 * ((1-b) + b * (float(dl)/float(avdl)))            \n",
    "\n",
    "def score_BM25(n, fq, N, dl, avgdl): # computes BM25 for 1 query word and 1 document\n",
    "    \n",
    "    k1 = 2.0\n",
    "    K = compute_K(dl, avgdl)\n",
    "    IDF = log((N - n + 0.5) / (n + 0.5))\n",
    "    frac = ((k1 + 1) * fq) / (K + fq)\n",
    "    return IDF * frac\n",
    "\n",
    "\n",
    "# ------------- find document matches (has at least 1 word from query) ------------- \n",
    "def match_docs(query, reverse_ids): # find document matches (has at least 1 word from query)\n",
    "    matching_docs = []\n",
    "    for word in final_query:\n",
    "        if word in reverse_ids:\n",
    "            matches = reverse_ids[word]\n",
    "            for i in matches:\n",
    "                if i not in matching_docs:\n",
    "                    matching_docs.append(i)\n",
    "    \n",
    "    return matching_docs\n",
    "\n",
    "\n",
    "# ------------- lemmatized text -> actual text + link ------------- \n",
    "def get_actual_text(reverse_text_id, text_ids):\n",
    "    get_text = text_ids[reverse_text_id]\n",
    "    f4 = open('/Users/Sofia/Desktop/inza-vpered/texts/' + get_text, 'r') # machine-specific\n",
    "    content = f4.readlines()\n",
    "    actual_text = content[5]\n",
    "    \n",
    "    link_part = re.sub('.txt', '', get_text)\n",
    "    link = 'http://inza-vpered.ru/article/' + link_part\n",
    "    f4.close()\n",
    "    \n",
    "    return (actual_text, link)\n",
    "\n",
    "# ------------- compute Okapi BM25 -------------\n",
    "def get_BM25(text, final_query, all_texts): # for 1 text\n",
    "                                      \n",
    "    k1 = 2.0\n",
    "    b = 0.75\n",
    "    \n",
    "    N = len(all_texts)\n",
    "    avgdl = compute_avgdl(all_texts)\n",
    "    dl = len(text)\n",
    "\n",
    "    sum_score = 0\n",
    "    \n",
    "    for q in final_query: # for each word in query\n",
    "        n = compute_n(q, reverse_ids) # num of texts that have qi (qi - word in query)\n",
    "        fq = compute_fq(q, text) # frequency of word q in document\n",
    "        score = score_BM25(n, fq, N, dl, avgdl) # computes BM25 for 1 query word and 1 document        \n",
    "        sum_score += score\n",
    "\n",
    "    return sum_score\n",
    "\n",
    "# ------------- final sorting function ------------- \n",
    "def BM25_sort(query, path_to_collection, texts_and_ids, reverse_ids):\n",
    "    \n",
    "    #texts_and_ids = lemmatize_collection(path)\n",
    "    all_texts = texts_and_ids[0]\n",
    "    text_ids = texts_and_ids[1]\n",
    "\n",
    "    #reverse_ids = reverse_id(all_texts)    \n",
    "    final_query = prepare_query(query)\n",
    "    \n",
    "    matches = match_docs(final_query, reverse_ids)\n",
    "    docs_and_scores = {}\n",
    "    \n",
    "    for match in matches: # match -- id\n",
    "        text = all_texts[match]\n",
    "        score = get_BM25(text, final_query, all_texts)\n",
    "        \n",
    "        actual_text = get_actual_text(match, text_ids)\n",
    "        docs_and_scores[actual_text] = score\n",
    "        \n",
    "    sorted_matches = sorted(docs_and_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # return only texts\n",
    "    \n",
    "    return sorted_matches\n",
    "    \n",
    "path = '/Users/Sofia/Desktop/inza-vpered/texts' # machine-specific\n",
    "texts_and_ids = lemmatize_collection(path)\n",
    "reverse_ids = reverse_id(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.85562869604411\n",
      "http://inza-vpered.ru/article/59687\n",
      "\n",
      "\n",
      "5.671064209516752\n",
      "http://inza-vpered.ru/article/116767\n",
      "\n",
      "\n",
      "5.490277707525085\n",
      "http://inza-vpered.ru/article/74388\n",
      "\n",
      "\n",
      "4.62715638498682\n",
      "http://inza-vpered.ru/article/47824\n",
      "\n",
      "\n",
      "4.3675683689987785\n",
      "http://inza-vpered.ru/article/47061\n",
      "\n",
      "\n",
      "4.301678404413293\n",
      "http://inza-vpered.ru/article/1179\n",
      "\n",
      "\n",
      "4.093279754782544\n",
      "http://inza-vpered.ru/article/91903\n",
      "\n",
      "\n",
      "3.6822646319557464\n",
      "http://inza-vpered.ru/article/50579\n",
      "\n",
      "\n",
      "3.6617760423097274\n",
      "http://inza-vpered.ru/article/111269\n",
      "\n",
      "\n",
      "2.9413238068199026\n",
      "http://inza-vpered.ru/article/40323\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'каникулы на новый год и рождество'\n",
    "\n",
    "sorted_matches = BM25_sort(query, path, texts_and_ids, reverse_ids)\n",
    "\n",
    "for match in sorted_matches[0:10]:\n",
    "    print(str(match[1]))\n",
    "    print(match[0][1])\n",
    "    print('\\n')\n",
    "\n",
    "#f5 = open('/Users/Sofia/Desktop/results.txt', 'w')\n",
    "#for text in sorted_matches:\n",
    "#    f5.write(str(text[1]))\n",
    "#    f5.write(text[0][1])\n",
    "#    f5.write(text[0][0])\n",
    "#    f5.write('\\n')\n",
    "#f5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
